{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genera un diccionario con todos los statutes del reto en s_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "directory = \"../dataset/Object_statutes\"\n",
    "# crea un diccionario con las sentencias en el que la clave es el nombre del fichero\n",
    "# sin el .txt y el valor es el contenido del fichero\n",
    "s_dict = {}\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        with open(filepath, \"r\") as file:\n",
    "            content = file.read()\n",
    "            key, value = filename.split(\".\")[0], content\n",
    "            # añadir al diccionario si el value no esta vacio\n",
    "            if value:\n",
    "                s_dict[key] = value\n",
    "            \n",
    "\n",
    "\n",
    "n_statutes = len(s_dict)  # numero de sentencias\n",
    "print(\"Numero de sentencias: \",n_statutes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"../dataset/Object_casedocs\"\n",
    "# crea un diccionario con los casos en el que la clave es el nombre del fichero\n",
    "# sin el .txt y el valor es el contenido del fichero\n",
    "c_dict = {}\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        with open(filepath, \"r\") as file:\n",
    "            content = file.read()\n",
    "            key, value = filename.split(\".\")[0], content\n",
    "            # añadir al diccionario si el value tiene contenido\n",
    "            if value:\n",
    "                c_dict[key] = value\n",
    "\n",
    "n_cases = len(c_dict)  # numero de casos\n",
    "print(\"Numero de casos: \",n_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea una lista a partir del diccionario de sentencias con la forma \"id\": clave, \"text\": valor\n",
    "sentences = [{\"id\": key, \"text\": value} for key, value in s_dict.items()]\n",
    "# Crea una lista a partir del diccionario de casos con la forma \"id\": clave, \"text\": valor\n",
    "cases = [{\"id\": key, \"text\": value} for key, value in c_dict.items()]\n",
    "\n",
    "import json\n",
    "# escribe cada una de las listas en un fichero json de sentencias \n",
    "with open(\"../dataset/sentences.json\", \"w\") as file:\n",
    "    json.dump(sentences, file, indent=4)\n",
    "\n",
    "# escribe cada una de las listas en 15 ficheros de casos separados en partes iguales\n",
    "# con el mismo numero de casos\n",
    "n = n_cases // 15\n",
    "for i in range(15):\n",
    "    with open(f\"../dataset/cases_{i}.json\", \"w\") as file:\n",
    "        json.dump(cases[i*n:(i+1)*n], file, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_file = \"../dataset/Query_doc.txt\"\n",
    "with open(query_file, \"r\") as file:\n",
    "    query = file.read()\n",
    "\n",
    "# a partir de query crea un diccionario dividendo por líneas. \n",
    "# La clave es la cadena antes del símbolo || y el valor lo que sigue hasta el final de la línea\n",
    "query_dict = {}\n",
    "for line in query.split(\"\\n\"):\n",
    "    if line != \"\":\n",
    "        key, value = line.split(\"||\")\n",
    "        query_dict[key] = value\n",
    "n_queries = len(query_dict)  # numero de queries\n",
    "print(\"Numero de queries: \",n_queries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "\n",
    "file_path = \"../dataset/relevance_judgments_statutes.txt\"\n",
    "factor = 4 # factor de multiplicación de sentencias no relevantes\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "# crea un diccionario con las sentencias relevantes para cada query\n",
    "# la clave es el nombre de la query y el valor es una lista con las sentencias relevantes\n",
    "# cada línea tiene la forma \"AILA_Q1 Q0 S1 1\" Q0 es un separador, S1 es la sentencia y \n",
    "# 1 si es relevante, 0 cero si no lo es\n",
    "\n",
    "s_relevance_dict = {} # diccionario con las sentencias relevantes indexadas por query\n",
    "for line in content.split(\"\\n\"):\n",
    "    # si line termina en un 1, entonces es relevante\n",
    "    if line.endswith(\"1\"):\n",
    "        key, value = line.split(\" \")[0], line.split(\" \")[2]\n",
    "        if key in s_relevance_dict:\n",
    "            if value in s_dict: # existen statutes como solución que no existen en el dict ej S58\n",
    "                s_relevance_dict[key].append(value)\n",
    "        else:\n",
    "            if value in s_dict: # existen statutes como solución que no existen en el dict ej S58\n",
    "                s_relevance_dict[key] = [value]\n",
    "\n",
    "# imprime por pantalla la query que tiene más sentencias relevantes y el número de sentencias relevantes\n",
    "max_key = max(s_relevance_dict, key=lambda x: len(s_relevance_dict[x]))\n",
    "print(\"Query con más sentencias relevantes: \",max_key)\n",
    "print(\"Número de sentencias relevantes: \",len(s_relevance_dict[max_key]))\n",
    "\n",
    "\n",
    "# s_query_dict es un diccionario con todas las sentencias relevantes y no relevantes\n",
    "# indexadas por query, y con un número de sentencias no relevantes igual a \"factor\" por número\n",
    "# de sentencias relevantes. Es decir, si para una query hay 3 sentencias relevantes,\n",
    "# y factor es 4, entonces el número de sentencias no relevantes será 3*4 = 12.\n",
    "# y el total para esa query será 15.\n",
    "\n",
    "s_query_dict = copy.deepcopy(s_relevance_dict) # copia el diccionario de sentencias relevantes\n",
    "\n",
    "# crea una lista con las claves de s_dict\n",
    "s_keys = list(s_dict.keys())\n",
    "for key in s_query_dict:\n",
    "    N_statutes_check = factor * len(s_query_dict[key]) # número de sentencias no relevantes a añadir\n",
    "    # baraja s_keys para no tomar siempre las mismas de relleno\n",
    "    random.shuffle(s_keys)\n",
    "    while len(s_query_dict[key]) < N_statutes_check:\n",
    "        for s in s_keys:\n",
    "            if s not in s_query_dict[key]:\n",
    "                s_query_dict[key].append(s)\n",
    "                if len(s_query_dict[key]) == N_statutes_check:\n",
    "                    break\n",
    "\n",
    "\n",
    "# baraja los elementos de la lista de valores de cada entrada de s_query_dict\n",
    "# para que no estén siempre al principio las sentencias relevantes\n",
    "for key in s_query_dict:\n",
    "    random.shuffle(s_query_dict[key])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "\n",
    "file_path = \"../dataset/relevance_judgments_priorcases.txt\"\n",
    "factor = 4 # factor de multiplicación de casos no relevantes\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "# crea un diccionario con los casos relevantes para cada query\n",
    "# la clave es el nombre de la query y el valor es una lista con las casos relevantes\n",
    "# cada línea tiene la forma \"AILA_Q1 Q0 C1 1\" Q0 es un separador, C1 es el caso y \n",
    "# 1 si es relevante, 0 cero si no lo es\n",
    "\n",
    "c_relevance_dict = {} # diccionario con los casos relevantes indexadas por query\n",
    "for line in content.split(\"\\n\"):\n",
    "    # si line termina en un 1, entonces es relevante\n",
    "    if line.endswith(\"1\"):\n",
    "        key, value = line.split(\" \")[0], line.split(\" \")[2]\n",
    "        if key in c_relevance_dict:\n",
    "            if value in c_dict: # existen casos como solución que podrían no existir en el dict\n",
    "                c_relevance_dict[key].append(value)\n",
    "        else:\n",
    "            if value in c_dict: # existen statutes como solución que no existen en el dict ej S58\n",
    "                c_relevance_dict[key] = [value]\n",
    "\n",
    "\n",
    "# c_query_dict es un diccionario con todos las casos relevantes y no relevantes\n",
    "# indexadas por query, y con un número de casos no relevantes igual a \"factor\" por número\n",
    "# de casos relevantes. Es decir, si para una query hay 3 casos relevantes,\n",
    "# y factor es 4, entonces el número de casos no relevantes será 3*4 = 12.\n",
    "# y el total para esa query será 15.\n",
    "\n",
    "c_query_dict = copy.deepcopy(c_relevance_dict) # copia el diccionario de casos relevantes\n",
    "\n",
    "# crea una lista con las claves de c_dict\n",
    "c_keys = list(c_dict.keys())\n",
    "for key in c_query_dict:\n",
    "    N_cases_check = factor * len(c_query_dict[key]) # número de sentencias no relevantes a añadir\n",
    "    # baraja s_keys para no tomar siempre las mismas de relleno\n",
    "    random.shuffle(c_keys)\n",
    "    while len(c_query_dict[key]) < N_cases_check:\n",
    "        for c in c_keys:\n",
    "            if c not in c_query_dict[key]:\n",
    "                c_query_dict[key].append(c)\n",
    "                if len(c_query_dict[key]) == N_cases_check:\n",
    "                    break\n",
    "\n",
    "\n",
    "# baraja los elementos de la lista de valores de cada entrada de c_query_dict\n",
    "# para que no estén siempre al principio las sentencias relevantes\n",
    "for key in c_query_dict:\n",
    "    random.shuffle(c_query_dict[key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generamos el prompt para los status\n",
    "query = \"AILA_Q5\" # query a buscar\n",
    "n_relevant = len(s_relevance_dict[query]) # número de sentencias relevantes\n",
    "\n",
    "\n",
    "ask_for_query = \"\\n\".join(f\"Statute index: {statute}\\n{s_dict[statute]}\" \n",
    "                       for statute in s_query_dict[query])\n",
    "\n",
    "# print(ask_for_query)\n",
    "\n",
    "prompt_template = f\"\"\"\n",
    "<s>[INST] Given the following information about statutes from Indian law, please indicate which ones \n",
    "are the {n_relevant} most relevant for the described situation that had led to filing a case in an Indian court of law. \n",
    "Statutes are considered relevant to a situation if they discusses a situation similar to that in the query, \n",
    "as judged by law experts.\n",
    "\n",
    "\n",
    "Statutes:\n",
    "{ask_for_query}\n",
    "\n",
    "Situation: {query_dict[query]}\n",
    "[/INST]\n",
    "\"\"\"\n",
    "print(prompt_template)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos el prompt para los status\n",
    "query = \"AILA_Q5\" # query a buscar\n",
    "n_relevant = len(c_relevance_dict[query]) # número de sentencias relevantes\n",
    "\n",
    "\n",
    "ask_for_query = \"\\n\".join(f\"Casedoc index: {case}\\n{c_dict[case]}\" \n",
    "                       for case in c_query_dict[query])\n",
    "\n",
    "# print(ask_for_query)\n",
    "\n",
    "prompt_template = f\"\"\"\n",
    "<s>[INST] Given the following information about cases from Indian law, please indicate which ones \n",
    "are the {n_relevant} most relevant for the described situation that had led to filing a case in an Indian court of law. \n",
    "Cases are considered relevant to a situation if they discusses a situation similar to that in the query, \n",
    "as judged by law experts.\n",
    "\n",
    "\n",
    "Cases:\n",
    "{ask_for_query}\n",
    "\n",
    "Situation: {query_dict[query]}\n",
    "[/INST]\n",
    "\"\"\"\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an expert in law.\"},\n",
    "    {\"role\": \"user\",  \"content\": prompt_template},\n",
    "]\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=5000,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "\n",
    "# Función que recibe una cadena con un nombre de modelo y un prompt, y devuelve el resultado\n",
    "# de la conversación con el modelo\n",
    "def ask_OpenAI(modelo, prompt):\n",
    "    \n",
    "    clave_api = os.getenv('OPENAI_API_KEY') \n",
    "\n",
    "    client = OpenAI(\n",
    "        api_key=clave_api\n",
    "    )\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': 'You are an expert in Indian law and legal cases.'},\n",
    "            {'role': 'user', 'content': prompt}\n",
    "        ],\n",
    "        #model=\"gpt-3.5-turbo\",\n",
    "        model=modelo,\n",
    "        temperature=0.7 # más cercano a 0 es más determinista, más cercano a 1 es creativo\n",
    "    )\n",
    "    # retorno de la función get_chat_completion\n",
    "    return chat_completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = \"gpt-4o\" \n",
    "\n",
    "respuesta = ask_OpenAI(modelo, prompt_template)\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "file_path = \"../resultados\"\n",
    "\n",
    "# para cada sentencia relevante de la query busca si está contenida en el resultado\n",
    "# de la conversación, y calcula el número de aciertos\n",
    "modelo = \"gpt-4o\" \n",
    "\n",
    "# recorre todas las queries y hace la consulta con un conjunto de sentencias relevantes y no relevantes\n",
    "for query in query_dict:\n",
    "    # imprime la query que se está procesando\n",
    "    print(\"Procesando la query \", query)\n",
    "    # imprime las sentencias relevantes de la query\n",
    "    print(\"Sentencias relevantes de la query \", s_relevance_dict[query])\n",
    "    # imprime las sentencias relevantes y no relevantes de la query\n",
    "    print(\"Sentencias relevantes y no relevantes de la query \", s_query_dict[query])\n",
    "\n",
    "    n_relevant = len(s_relevance_dict[query]) # número de sentencias relevantes\n",
    "    ask_for_query = \"\\n\".join(f\"Statute index: {statute}\\n{s_dict[statute]}\" \n",
    "                        for statute in s_query_dict[query])\n",
    "    # generamos el prompt\n",
    "    prompt_template = f\"\"\"\n",
    "    <s>[INST] Given the following information about statutes from Indian law, please indicate which ones \n",
    "    are the {n_relevant} most relevant for the described situation that had led to filing a case in an Indian court of law. \n",
    "    Statutes are considered relevant to a situation if they discusses a situation similar to that in the query, \n",
    "    as judged by law experts.\n",
    "\n",
    "\n",
    "    Statutes:\n",
    "    {ask_for_query}\n",
    "\n",
    "    Situation: {query_dict[query]}\n",
    "    [/INST]\n",
    "    \"\"\"\n",
    "    respuesta = ask_OpenAI(modelo, prompt_template)\n",
    "    # calculamos la precisión\n",
    "    aciertos = 0\n",
    "    for s in s_relevance_dict[query]:\n",
    "        if s in respuesta:\n",
    "            aciertos += 1\n",
    "    # calcula el porcentaje de aciertos sobre el número de sentencias relevantes y no relevantes de la query\n",
    "    precision = aciertos / len(s_relevance_dict[query]) * 100\n",
    "\n",
    "    # crea un fichero con el resultado de la consulta\n",
    "    resultado = f\"\"\"\n",
    "    CONSULTA REALIZADA:\n",
    "    {prompt_template}\n",
    "    ----------------------------------------------------\n",
    "\n",
    "    RESPUESTA DE {modelo}:\n",
    "    {respuesta}\n",
    "    ----------------------------------------------------\n",
    "\n",
    "    LISTA DE SENTENCIAS RELEVANTES PARA LA QUERY {query}:\n",
    "    {str(s_relevance_dict[query])}\n",
    "    ----------------------------------------------------\n",
    "\n",
    "    LISTA DE SENTENCIAS RELEVANTES Y NO RELEVANTES PARA LA QUERY {query}:\n",
    "    {str(s_query_dict[query])}\n",
    "    ----------------------------------------------------\n",
    "\n",
    "    PRECISIÓN {query}: {precision}%\n",
    "    \"\"\"\n",
    "\n",
    "    file_name = file_path + f\"/chat_completion_{query}.txt\"\n",
    "    # Guarda el resultado en un fichero, y crealo si no existe o truncalo si existe\n",
    "    with open(file_name, \"w\") as file:\n",
    "        file.write(resultado)\n",
    "    # cierra el fichero\n",
    "    file.close()\n",
    "    print(\"Se ha procesado la query \", query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "file_path = \"../resultados/queries_Nfactor_cases\"\n",
    "\n",
    "# para cada caso relevante de la query busca si está contenid en el resultado\n",
    "# de la conversación, y calcula el número de aciertos\n",
    "modelo = \"gpt-4o\" \n",
    "\n",
    "# recorre todas las queries y hace la consulta con un conjunto de sentencias relevantes y no relevantes\n",
    "for query in query_dict:\n",
    "    # comprueba si existe el fichero con el resultado de la consulta\n",
    "    if os.path.exists(file_path + f\"/chat_completion_{query}.txt\"):\n",
    "        # si existe, pasa a la siguiente query\n",
    "        print(\"Ya se ha procesado la query \", query)\n",
    "        continue\n",
    "\n",
    "    # imprime la query que se está procesando\n",
    "    print(\"Procesando la query \", query)\n",
    "    # imprime los casos relevantes de la query\n",
    "    print(\"Casos relevantes de la query \", c_relevance_dict[query])\n",
    "    # imprime las sentencias relevantes y no relevantes de la query\n",
    "    print(\"Casos relevantes y no relevantes de la query \", c_query_dict[query])\n",
    "\n",
    "    n_relevant = len(c_relevance_dict[query]) # número de casos relevantes\n",
    "    ask_for_query = \"\\n\".join(f\"Case index: {casedoc}\\n{c_dict[casedoc]}\" \n",
    "                        for casedoc in c_query_dict[query])\n",
    "    # generamos el prompt\n",
    "    prompt_template = f\"\"\"\n",
    "    <s>[INST] Given the following information about cases from Indian law, please indicate which ones \n",
    "    are the {n_relevant} most relevant for the described situation that had led to filing a case in an Indian court of law. \n",
    "    Cases are considered relevant to a situation if they discusses a situation similar to that in the query, \n",
    "    as judged by law experts.\n",
    "\n",
    "\n",
    "    Cases:\n",
    "    {ask_for_query}\n",
    "\n",
    "    Situation: {query_dict[query]}\n",
    "    [/INST]\n",
    "    \"\"\"\n",
    "    respuesta = ask_OpenAI(modelo, prompt_template)\n",
    "    # calculamos la precisión\n",
    "    aciertos = 0\n",
    "    for c in c_relevance_dict[query]:\n",
    "        if c in respuesta:\n",
    "            aciertos += 1\n",
    "    # calcula el porcentaje de aciertos sobre el número de sentencias relevantes y no relevantes de la query\n",
    "    precision = aciertos / len(c_relevance_dict[query]) * 100\n",
    "\n",
    "    # crea un fichero con el resultado de la consulta\n",
    "    resultado = f\"\"\"\n",
    "    CONSULTA REALIZADA:\n",
    "    {prompt_template}\n",
    "    ----------------------------------------------------\n",
    "\n",
    "    RESPUESTA DE {modelo}:\n",
    "    {respuesta}\n",
    "    ----------------------------------------------------\n",
    "\n",
    "    LISTA DE SENTENCIAS RELEVANTES PARA LA QUERY {query}:\n",
    "    {str(c_relevance_dict[query])}\n",
    "    ----------------------------------------------------\n",
    "\n",
    "    LISTA DE SENTENCIAS RELEVANTES Y NO RELEVANTES PARA LA QUERY {query}:\n",
    "    {str(c_query_dict[query])}\n",
    "    ----------------------------------------------------\n",
    "\n",
    "    PRECISIÓN {query}: {precision}%\n",
    "    \"\"\"\n",
    "\n",
    "    file_name = file_path + f\"/chat_completion_{query}.txt\"\n",
    "    # Guarda el resultado en un fichero, y crealo si no existe o truncalo si existe\n",
    "    with open(file_name, \"w\") as file:\n",
    "        file.write(resultado)\n",
    "    # cierra el fichero\n",
    "    file.close()\n",
    "    print(\"Se ha procesado la query \", query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../resultados\"\n",
    "modelo = \"gpt-4o\" \n",
    "\n",
    "\n",
    "# recorre todas las queries, y para cada querie recorre todas las sentencias.\n",
    "# imprimiendo por pantalla el par query-sentencia\n",
    "s_relevance_calc_dict = {} # diccionario con las sentencias relevantes calculadas\n",
    "\n",
    "for q in query_dict:\n",
    "    s_relevance_calc_dict[q] = []\n",
    "    for s in s_dict:\n",
    "        prompt_template = f\"\"\"\n",
    "        <s>[INST] Given the following information about statutes from Indian law, please indicate if the indicated\n",
    "        statute is relevant for the described situation that had led to filing a case in an Indian court of law, answering\n",
    "        with a yes or no. Statutes are considered relevant to a situation if they discusses a situation similar to that in the query,\n",
    "        as judged by law experts.\n",
    "        \n",
    "        Statute: {s_dict[s]}\n",
    "\n",
    "        Situation: {query_dict[q]}\n",
    "        [/INST]\n",
    "        \"\"\"\n",
    "        respuesta = ask_OpenAI(modelo, prompt_template)\n",
    "        print(f\"Query: {q} - Sentencia: {s} - Respuesta: {respuesta}\")\n",
    "        if \"Yes\" in respuesta:\n",
    "            s_relevance_calc_dict[q].append(s)\n",
    "\n",
    "# construye una tabla de resultados con las sentencias relevantes calculadas\n",
    "# en la que cada línea tiene el nombre de la query, y separadas por coma\n",
    "# las sentencias relevantes calculadas\n",
    "table = \"Leyes relevantes calculadas por la IA para cada query\\n\"\n",
    "for q in s_relevance_calc_dict:\n",
    "    table += f\"{q},{','.join(s_relevance_calc_dict[q])}\\n\"\n",
    "# imprime la tabla y la guarda en un fichero\n",
    "print(table)\n",
    "file_name = file_path + f\"/leyes_relevantes_calculadas_para_todas_queries.txt\"\n",
    "with open(file_name, \"w\") as file:\n",
    "    file.write(table)\n",
    "file.close()\n",
    "\n",
    "# construye una tabla de resultados con todas las queries, en el que la primera columna\n",
    "# es el nombre de la query, la segunda columna el número de aciertos positivos, es decir,\n",
    "# aquellos statutes que se encuentran en s_relevance_dict y en s_relevance_calc_dict\n",
    "# la tercera columna es el número de aciertos negativos, es decir, aquellos statutes que\n",
    "# no se encuentran en s_relevance_dict y tampoco en s_relevance_calc_dict, la cuarta columna\n",
    "# es el número de falsos positivos, es decir, aquellos statutes que se encuentran en\n",
    "# s_relevance_calc_dict pero no en s_relevance_dict, y la quinta columna es el número de\n",
    "# falsos negativos, es decir, aquellos statutes que se encuentran en s_relevance_dict\n",
    "# pero no en s_relevance_calc_dict.\n",
    "table = \"Comparación de resultados para todas las queries\\n\"\n",
    "table += \"Query,Aciertos positivos,Aciertos negativos,Falsos positivos,Falsos negativos\\n\"\n",
    "for q in s_relevance_dict:\n",
    "    aciertos_positivos = len(set(s_relevance_dict[q]) & set(s_relevance_calc_dict[q]))\n",
    "    falsos_positivos = len(set(s_relevance_calc_dict[q]) - set(s_relevance_dict[q]))\n",
    "    falsos_negativos = len(set(s_relevance_dict[q]) - set(s_relevance_calc_dict[q]))\n",
    "    aciertos_negativos = len(s_dict) - aciertos_positivos - falsos_positivos - falsos_negativos\n",
    "    table += f\"{q},{aciertos_positivos},{aciertos_negativos},{falsos_positivos},{falsos_negativos}\\n\"\n",
    "\n",
    "# imprime la tabla y la guarda en un fichero\n",
    "print(table)\n",
    "file_name = file_path + f\"/comparacion_resultados_statutes_para_todas_queries.txt\"\n",
    "with open(file_name, \"w\") as file:\n",
    "    file.write(table)\n",
    "file.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../resultados\"\n",
    "modelo = \"gpt-4o\" \n",
    "\n",
    "\n",
    "# recorre todas las queries, y para cada querie recorre todos los casedocs.\n",
    "# imprimiendo por pantalla el par query-casedoc\n",
    "c_relevance_calc_dict = {} # diccionario con las sentencias relevantes calculadas\n",
    "\n",
    "for q in query_dict:\n",
    "    c_relevance_calc_dict[q] = []\n",
    "    for c in c_dict:\n",
    "        prompt_template = f\"\"\"\n",
    "        <s>[INST] Given the following information about cases from Indian law, please indicate if the indicated\n",
    "        case is relevant for the described situation that had led to filing a case in an Indian court of law, answering\n",
    "        with a yes or no. Cases are considered relevant to a situation if they discusses a situation similar to that in the query,\n",
    "        as judged by law experts.\n",
    "        \n",
    "        Case: {c_dict[c]}\n",
    "\n",
    "        Situation: {query_dict[q]}\n",
    "        [/INST]\n",
    "        \"\"\"\n",
    "        respuesta = ask_OpenAI(modelo, prompt_template)\n",
    "        # genera una cadena con la consulta y la respuesta\n",
    "        consultaYrespuesta = f\"Query: {q} - Casedoc: {c} - Respuesta: {respuesta}\"\n",
    "        print(consultaYrespuesta)\n",
    "        if \"Yes\" in respuesta:\n",
    "            c_relevance_calc_dict[q].append(c)\n",
    "        # guarda la respuesta en un fichero\n",
    "        file_name = file_path + f\"/queries_casos/respuesta_{q}_{c}.txt\"\n",
    "        with open(file_name, \"w\") as file:\n",
    "            file.write(consultaYrespuesta)\n",
    "\n",
    "\n",
    "# construye una tabla de resultados con las casos relevantes calculadas\n",
    "# en la que cada línea tiene el nombre de la query, y separadas por coma\n",
    "# los casos relevantes calculados\n",
    "table = \"Casos relevantes calculados por la IA para cada query\\n\"\n",
    "for q in c_relevance_calc_dict:\n",
    "    table += f\"{q},{','.join(c_relevance_calc_dict[q])}\\n\"\n",
    "# imprime la tabla y la guarda en un fichero\n",
    "print(table)\n",
    "file_name = file_path + f\"/casos_relevantes_calculados_para_todas_queries.txt\"\n",
    "with open(file_name, \"w\") as file:\n",
    "    file.write(table)\n",
    "file.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "file_path = \"../resultados\"\n",
    "result_path = file_path + f\"/queries_casos/\"\n",
    "\n",
    "# lee todos los ficheros de result_path y los imprime por pantalla y construye una tabla de resultados\n",
    "# la query de referencia, el casedoc de referencia y la respuesta. La respuesta es Yes si el casedoc\n",
    "# es relevante para la query, y No si no lo es. En la tabla poner un 1 si la respuesta es Yes y un 0 si\n",
    "# la respuesta es No. El nombre de la query y el casedoc se obtiene del nombre del fichero. El de la query\n",
    "# es el que se encuentra entre dos _ y el del casedoc es el que se encuentra entre el segundo _ y el .txt\n",
    "\n",
    "# creo un diccionario de casos relevantes para cada query a partir de los ficheros de resultados\n",
    "c_relevance_calc_dict = {} # diccionario con las sentencias relevantes calculadas\n",
    "table = \"\"\n",
    "for file_name in os.listdir(result_path):\n",
    "    with open(result_path + file_name, \"r\") as file:\n",
    "        content = file.read()\n",
    "        # obtiene la query y el casedoc del nombre del fichero\n",
    "        query = file_name.split(\"_\")[2]\n",
    "        casedoc = file_name.split(\"_\")[3].split(\".\")[0]\n",
    "        # obtiene la respuesta\n",
    "        respuesta = \"1\" if \"Yes\" in content else \"0\"\n",
    "        table += f\"AILA_{query} Q0 {casedoc} {respuesta}\\n\"\n",
    "        if respuesta == \"1\":\n",
    "            query = f\"AILA_{query}\"\n",
    "            if query in c_relevance_calc_dict:\n",
    "                c_relevance_calc_dict[query].append(casedoc)\n",
    "            else:\n",
    "                c_relevance_calc_dict[query] = [casedoc]\n",
    "        # print(f\"Query: {query} - Casedoc: {casedoc} - Respuesta: {respuesta}\")  \n",
    "# imprime la tabla y la guarda en un fichero\n",
    "# print(table)\n",
    "file_name = file_path + f\"/comparacion_resultados_casedocs_para_todas_queries.txt\"\n",
    "with open(file_name, \"w\") as file:\n",
    "    file.write(table)\n",
    "file.close()\n",
    "\n",
    "# construye una tabla de resultados con todas las queries, en el que la primera columna\n",
    "# es el nombre de la query, la segunda columna el número de aciertos positivos, es decir,\n",
    "# aquellos casos que se encuentran en c_relevance_dict y en c_relevance_calc_dict\n",
    "# la tercera columna es el número de aciertos negativos, es decir, aquellos casos que\n",
    "# no se encuentran en c_relevance_dict y tampoco en c_relevance_calc_dict, la cuarta columna\n",
    "# es el número de falsos positivos, es decir, aquellos statutes que se encuentran en\n",
    "# c_relevance_calc_dict pero no en c_relevance_dict, y la quinta columna es el número de\n",
    "# falsos negativos, es decir, aquellos statutes que se encuentran en c_relevance_dict\n",
    "# pero no en c_relevance_calc_dict.\n",
    "table = \"Comparación de resultados para todas las queries\\n\"\n",
    "table += \"Query,Aciertos positivos,Aciertos negativos,Falsos positivos,Falsos negativos\\n\"\n",
    "for q in c_relevance_calc_dict:\n",
    "    print(\"solucion: \",c_relevance_dict[q])\n",
    "    print(\"calculado: \",c_relevance_calc_dict[q])\n",
    "    aciertos_positivos = len(set(c_relevance_dict[q]) & set(c_relevance_calc_dict[q]))\n",
    "    falsos_positivos = len(set(c_relevance_calc_dict[q]) - set(c_relevance_dict[q]))\n",
    "    falsos_negativos = len(set(c_relevance_dict[q]) - set(c_relevance_calc_dict[q]))\n",
    "    aciertos_negativos = len(c_dict) - aciertos_positivos - falsos_positivos - falsos_negativos\n",
    "    table += f\"{q},{aciertos_positivos},{aciertos_negativos},{falsos_positivos},{falsos_negativos}\\n\"\n",
    "\n",
    "# imprime la tabla y la guarda en un fichero\n",
    "print(table)\n",
    "file_name = file_path + f\"/tabla_resultados_casedocs_para_todas_queries.txt\"\n",
    "with open(file_name, \"w\") as file:\n",
    "    file.write(table)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = \"../resultados/exp5_statutes\"\n",
    "modelo = \"gpt-4o\"\n",
    "lista_queries_test = [\"AILA_Q5\",\"AILA_Q12\",\"AILA_Q17\",\"AILA_Q28\"]\n",
    "# para cada query de la lista de queries de test imprime el nombre por pantalla\n",
    "for query in lista_queries_test:\n",
    "    # agrega el nombre de la query al de file_path\n",
    "    file_path += f\"/{query}\"\n",
    "    # crea el directorio si no existe\n",
    "    if not os.path.exists(file_path):\n",
    "        os.makedirs(file_path)\n",
    "    # imprime los statutes relevantes para la query de ejemplo\n",
    "    print(\"Statutes relevantes para la query de ejemplo \",query)\n",
    "    print(s_relevance_dict[query])\n",
    "    grupo_filtro1 =[]  # grupo de statutes seleccionadas de cada grupo inicial\n",
    "    grupo_filtro2 =[]  # grupo de statutes seleccionadas del grupo_filtro1\n",
    "\n",
    "    # recorre todas las statutes y crea grupos de 10 statutes y almacenalas\n",
    "    # en un array de grupos\n",
    "    # imprime los grupos de statutes\n",
    "    #n = 10\n",
    "    n = 4\n",
    "    groups = [list(s_dict.keys())[i:i + n] for i in range(0, len(s_dict), n)]\n",
    "    for i, group in enumerate(groups):\n",
    "        print(f\"Grupo {i}: {group}\")\n",
    "        ask_for_query = \"\\n\".join(f\"Statute index: {s}\\n{s_dict[s]}\" \n",
    "                                for s in group) \n",
    "        # generamos el prompt\n",
    "        # n_relevant = 3 # número de sentencias relevantes que cogemos en este experimentos\n",
    "        n_relevant = 2 # número de sentencias relevantes que cogemos en este experimentos\n",
    "        prompt_template = f\"\"\"\n",
    "        <s>[INST] Given the following information about statutes from Indian law, please indicate which ones \n",
    "        are the {n_relevant} most relevant for the described situation that had led to filing a case in an Indian court of law. \n",
    "        Statutes are considered relevant to a situation if they discusses a situation similar to that in the query, \n",
    "        as judged by law experts.\n",
    "\n",
    "\n",
    "        Statutes:\n",
    "        {ask_for_query}\n",
    "\n",
    "        Situation: {query_dict[query]}\n",
    "        [/INST]\n",
    "        \"\"\"\n",
    "        respuesta = ask_OpenAI(modelo, prompt_template)\n",
    "        fichero = file_path + f\"/respuesta_query_{query}_grupo_{i}.txt\"\n",
    "        # graba la respuesta al fichero\n",
    "        with open(fichero, \"w\") as file:\n",
    "            file.write(respuesta)\n",
    "        # imprime la respuesta    \n",
    "        # print(respuesta)\n",
    "        # para cada statute del grupo comprueba si está en la respuesta y si es así lo añade al grupo_filtro1\n",
    "        for s in group:\n",
    "            if s in respuesta:\n",
    "                print(f\"Statute {s} is relevant del grupo {i}\")\n",
    "                grupo_filtro1.append(s)\n",
    "    # baraja el grupo de statutes seleccionadas\n",
    "    random.shuffle(grupo_filtro1)\n",
    "    # a partir del grupo_filtro1 crea grupos de 10 statutes y los imprime\n",
    "    # n = 10  \n",
    "    n = 3\n",
    "    groups = [grupo_filtro1[i:i + n] for i in range(0, len(grupo_filtro1), n)]\n",
    "    for i, group in enumerate(groups):\n",
    "        print(f\"Grupo {i}: {group}\")\n",
    "        ask_for_query = \"\\n\".join(f\"Statute index: {s}\\n{s_dict[s]}\" \n",
    "                                for s in group) \n",
    "        # generamos el prompt\n",
    "        # n_relevant = 2 # número de sentencias relevantes que cogemos en este experimentos\n",
    "        n_relevant = 1 # número de sentencias relevantes que cogemos en este experimentos\n",
    "        prompt_template = f\"\"\"\n",
    "        <s>[INST] Given the following information about statutes from Indian law, please indicate which ones\n",
    "        are the {n_relevant} most relevant for the described situation that had led to filing a case in an Indian court of law.\n",
    "        Statutes are considered relevant to a situation if they discusses a situation similar to that in the query,\n",
    "        as judged by law experts.\n",
    "\n",
    "        \n",
    "        Statutes:\n",
    "        {ask_for_query}\n",
    "\n",
    "        Situation: {query_dict[query]}\n",
    "        [/INST]\n",
    "        \"\"\"\n",
    "        respuesta = ask_OpenAI(modelo, prompt_template)\n",
    "        fichero = file_path + f\"/respuesta_query_{query}_grupo_filtro2_{i}.txt\"\n",
    "        # graba la respuesta al fichero\n",
    "        with open(fichero, \"w\") as file:\n",
    "            file.write(respuesta)\n",
    "        # imprime la respuesta\n",
    "        # print(respuesta)\n",
    "        # para cada statute del grupo comprueba si está en la respuesta y si es así lo añade al grupo_filtro2\n",
    "        for s in group:\n",
    "            if s in respuesta:\n",
    "                print(f\"Statute {s} is relevant del grupo {i}\")\n",
    "                grupo_filtro2.append(s)\n",
    "        \n",
    "\n",
    "    # baraja el grupo de statutes seleccionadas\n",
    "    random.shuffle(grupo_filtro2)\n",
    "    n_relevant = 5 # escogemos con una última consulta 5 statutes más relevantes\n",
    "    ask_for_query = \"\\n\".join(f\"Statute index: {s}\\n{s_dict[s]}\" \n",
    "                            for s in grupo_filtro2) \n",
    "    prompt_template = f\"\"\"\n",
    "        <s>[INST] Given the following information about statutes from Indian law, please indicate which ones\n",
    "        are the {n_relevant} most relevant for the described situation that had led to filing a case in an Indian court of law.\n",
    "        Statutes are considered relevant to a situation if they discusses a situation similar to that in the query,\n",
    "        as judged by law experts.\n",
    "\n",
    "        \n",
    "        Statutes:\n",
    "        {ask_for_query}\n",
    "\n",
    "        Situation: {query_dict[query]}\n",
    "        [/INST]\n",
    "        \"\"\"\n",
    "    respuesta = ask_OpenAI(modelo, prompt_template)\n",
    "    fichero = file_path + f\"/respuesta_query_{query}_solucionPropuesta.txt\"\n",
    "    # graba la respuesta al fichero\n",
    "    with open(fichero, \"w\") as file:\n",
    "        file.write(respuesta)\n",
    "    solucion_propuesta =[]\n",
    "    # para cada statute del grupo comprueba si está en la respuesta y si es así lo añade solucion_propuesta\n",
    "    for s in grupo_filtro2:\n",
    "        if s in respuesta:\n",
    "            print(f\"Statute {s} is relevant\")\n",
    "            solucion_propuesta.append(s)\n",
    "    print(\"Statutes relevantes para la query de ejemplo \",query)\n",
    "    print(s_relevance_dict[query])\n",
    "    print(\"Statutes propuestas como solucion: \",solucion_propuesta)\n",
    "    # calcula el número de aciertos\n",
    "    aciertos = len(set(s_relevance_dict[query]) & set(solucion_propuesta))\n",
    "    print(\"Número de aciertos: \",aciertos)\n",
    "    # calcula la precisión\n",
    "    precision = aciertos / len(s_relevance_dict[query]) * 100\n",
    "    print(\"Precisión: \",precision)\n",
    "    # crea un fichero con los resultados que incluya la query, las statutes relevantes, las statutes propuestas\n",
    "    # como solución y la precisión\n",
    "    resultado = f\"\"\"\n",
    "    Query: {query}\n",
    "    Statutes relevantes: {s_relevance_dict[query]}\n",
    "    Statutes propuestas como solución: {solucion_propuesta}\n",
    "    Precisión: {precision}\n",
    "    \"\"\"\n",
    "    fichero = file_path + f\"/resultado_query_{query}.txt\"\n",
    "    # graba el resultado en el fichero\n",
    "    with open(fichero, \"w\") as file:\n",
    "        file.write(resultado)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import tiktoken # módulo con funciones para trabajar con tokens\n",
    "# cojo el API_KEY de la variable de entorno\n",
    "mi_api_key = os.getenv('OPENAI_API_KEY') # configurada en activate del entorno $HOME/ULL-env/bin/activate\n",
    "\n",
    "# Configurar API Key de OpenAI\n",
    "openai_client = openai.OpenAI(api_key=mi_api_key)  # Nuevo cliente para OpenAI\n",
    "\n",
    "\n",
    "\n",
    "# los modelos usados de OpenAI nbo soportan mas de 8192 tokens\n",
    "# Como los textos no son muy largos (los casedocs y las statutes) optamos por trucar\n",
    "# los textos a 8192 tokens. Si fueran más largos habría que dividirlos en trozos\n",
    "# de 8192 tokens y hacer varias consultas, y luego unir los resultados. Existen varias\n",
    "# estrategias como sacar el vector promedio de los embeddings de cada trozo. En este caso\n",
    "# no es necesario porque los textos no son muy largos.\n",
    "\n",
    "#modelo_openAI=\"text-embedding-3-small\"\n",
    "modelo_openAI=\"text-embedding-3-large\"\n",
    "#modelo_openAI=\"text-embedding-ada-002\"\n",
    "\n",
    "# Función para obtener embeddings usando la API moderna de OpenAI\n",
    "# embeddings avanzados de OpenAI text-embedding-3-large y text-embedding-3-small\n",
    "# Requiere que esté definida modelo_openAI\n",
    "\n",
    "def get_embedding_OpenAI(text, model=modelo_openAI):\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "        MAX_TOKENS = 8192  # Límite del modelo\n",
    "        \n",
    "        tokens = encoding.encode(text)  # Tokenizar texto\n",
    "        \n",
    "        if len(tokens) > MAX_TOKENS:\n",
    "            print(f\"Warning: Texto con {len(tokens)} tokens excede el límite ({MAX_TOKENS}). Se truncará.\")\n",
    "            tokens = tokens[:MAX_TOKENS]  # Truncar al límite permitido\n",
    "            text = encoding.decode(tokens)  # Convertir tokens truncados a texto\n",
    "\n",
    "        response = openai_client.embeddings.create(input=[text], model=model)\n",
    "        return response.data[0].embedding\n",
    "    except openai.OpenAIError as e:\n",
    "        print(f\"Error en OpenAI al obtener embeddings del texto: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error desconocido: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Inicializar ChromaDB en memoria (sin persistencia en disco)\n",
    "# chroma_client = chromadb.EphemeralClient()\n",
    "# si se quiere hacer con persistencia en disco\n",
    "\n",
    "# descomentar la siguiente linea para embeddings de OpenAI\n",
    "#embedding_name = \"OpenAI\" # nombre del modelo de embeddings OpenAI\n",
    "\n",
    "# descomentar las siguientes 3 lineas para embeddings de HF model all-MiniLM-L6-v2\n",
    "#embedding_name = \"HF_MiniLM\" # nombre del modelo de embeddings MiniLM\n",
    "#nombre_modelo = \"all-MiniLM-L6-v2\"  # mas ligero pero menos preciso\n",
    "#model = SentenceTransformer(nombre_modelo)  \n",
    "\n",
    "# descomentar las siguientes 3 lineas para embeddings de HF model all-mpnet-base-v2\n",
    "#embedding_name = \"HF_mpnet\" # nombre del modelo de embeddings mpnet\n",
    "#nombre_modelo = \"sentence-transformers/all-mpnet-base-v2\"  # mas pesado pero mas preciso\n",
    "#model = SentenceTransformer(nombre_modelo)  \n",
    "\n",
    "# descomentar las siguientes 3 lineas para embeddings de HF model all-mpnet-base-v2- fine tunning\n",
    "embedding_name = \"HF_mpnet_finetunning\" # nombre del modelo de embeddings mpnet con fine tunning\n",
    "nombre_modelo = \"../modelos_fine_tunning/sentence-transformers/all-mpnet-base-v2_finetunning\"  # con finetunning del mpnet\n",
    "model = SentenceTransformer(nombre_modelo)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "path_DB = \"../chroma_db_\"+embedding_name\n",
    "chroma_client = chromadb.PersistentClient(path=path_DB)  \n",
    "collection = chroma_client.get_or_create_collection(name=embedding_name)\n",
    "\n",
    "# separa los statutes por un lado los identificadores, y por otros los textos\n",
    "statutes = list(s_dict.keys())\n",
    "texts = list(s_dict.values())\n",
    "\n",
    "# Generar y almacenar embeddings en ChromaDB\n",
    "# generar embeddings si el modelo es HF_MiniLM o HF_mpnet\n",
    "\n",
    "if embedding_name == \"HF_MiniLM\" or embedding_name == \"HF_mpnet\" or embedding_name == \"HF_mpnet_finetunning\":\n",
    "    # si no se recorre en un bucle como con OpenAI se procesan todos los textos\n",
    "    # de una vez aprovechando la capacidad de procesamiento paralelo\n",
    "    embeddings = model.encode(texts).tolist()  # Obtener embeddings de cada texto\n",
    "# sino generar embeddings con OpenAI\n",
    "elif embedding_name == \"OpenAI\":    \n",
    "    embeddings = [get_embedding_OpenAI(text) for text in texts]  # Obtener embeddings de cada texto\n",
    "\n",
    "collection.add(documents=texts, embeddings=embeddings, ids=statutes) # lo añadimos con su id de statute\n",
    "\n",
    "print(\"Embeddings almacenados correctamente en ChromaDB para embeddings\",embedding_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Inicializar ChromaDB en memoria (sin persistencia en disco)\n",
    "# chroma_client = chromadb.EphemeralClient()\n",
    "# si se quiere hacer con persistencia en disco\n",
    "\n",
    "# descomentar la siguiente linea para embeddings de OpenAI\n",
    "embedding_name = \"OpenAI\" # nombre del modelo de embeddings OpenAI\n",
    "\n",
    "# descomentar las siguientes 3 lineas para embeddings de HF model all-MiniLM-L6-v2\n",
    "#embedding_name = \"HF_MiniLM\" # nombre del modelo de embeddings MiniLM\n",
    "#nombre_modelo = \"all-MiniLM-L6-v2\"  # mas ligero pero menos preciso\n",
    "#model = SentenceTransformer(nombre_modelo)  \n",
    "\n",
    "# descomentar las siguientes 3 lineas para embeddings de HF model all-mpnet-base-v2\n",
    "#embedding_name = \"HF_mpnet\" # nombre del modelo de embeddings mpnet\n",
    "#nombre_modelo = \"sentence-transformers/all-mpnet-base-v2\"  # mas pesado pero mas preciso\n",
    "#model = SentenceTransformer(nombre_modelo)  \n",
    "\n",
    "path_DB = \"../chroma_db_casedocs_\"+embedding_name\n",
    "#path_DB = \"./chroma_db\"\n",
    "chroma_client = chromadb.PersistentClient(path=path_DB)  \n",
    "collection = chroma_client.get_or_create_collection(name=embedding_name)\n",
    "print(\"Colección de embeddings de casedocs creada correctamente\")\n",
    "\n",
    "# separa los statutes por un lado los identificadores, y por otros los textos\n",
    "casedocs = list(c_dict.keys())\n",
    "texts = list(c_dict.values())\n",
    "\n",
    "# Generar y almacenar embeddings en ChromaDB\n",
    "# generar embeddings si el modelo es HF_MiniLM o HF_mpnet\n",
    "embeddings = []\n",
    "if embedding_name == \"HF_MiniLM\" or embedding_name == \"HF_mpnet\":\n",
    "    # si no se recorre en un bucle como con OpenAI se procesan todos los textos\n",
    "    # de una vez aprovechando la capacidad de procesamiento paralelo\n",
    "    embeddings = model.encode(texts).tolist()  # Obtener embeddings de cada texto\n",
    "# sino generar embeddings con OpenAI\n",
    "elif embedding_name == \"OpenAI\":    \n",
    "    # crea una lista vacia de entradas del diccionario que no se han podido procesar\n",
    "    # para eliminarlas del diccionario\n",
    "    c_dict_sin_procesar = []\n",
    "    for c in c_dict:\n",
    "        # imprime la clave del casedoc que se está procesando        \n",
    "        # print(f\"Calculando embedding de casedoc: {c}\\n\")\n",
    "        # calcula el embedding de cada valor del diccionario y lo añades\n",
    "        # a la lista de embeddings\n",
    "        emb = get_embedding_OpenAI(c_dict[c])  # Obtener embeddings de cada texto\n",
    "        if emb is None:\n",
    "            print(f\"Error al calcular el embedding de casedoc: {c}\\n\")\n",
    "            # si no se ha podido calcular el embedding añade la clave a la lista\n",
    "            # de casedocs sin procesar\n",
    "            c_dict_sin_procesar.append(c)\n",
    "        else:   \n",
    "            embeddings.append(emb)\n",
    "            #print(f\"Embedding de casedoc {c} calculado correctamente. Tamaño actual {len(embeddings)}\\n\")\n",
    "    # elimina las claves sin procesar del diccionario\n",
    "    for c in c_dict_sin_procesar:\n",
    "        del c_dict[c]\n",
    "    # volver a generar las listas porque pueden haberse eliminados algunos casedocs\n",
    "    casedocs = list(c_dict.keys())\n",
    "    texts = list(c_dict.values())\n",
    "    # la siguiente linea era para no hacerlo en un bucle e imprimiendo los casos   \n",
    "    #embeddings = [get_embedding_OpenAI(text) for text in texts]  # Obtener embeddings de cada texto\n",
    "\n",
    "collection.add(documents=texts, embeddings=embeddings, ids=casedocs) # lo añadimos con su id de statute\n",
    "\n",
    "print(\"Embeddings de casedocs almacenados correctamente en ChromaDB para embeddings\",embedding_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Inicializar ChromaDB en memoria (sin persistencia en disco)\n",
    "# chroma_client = chromadb.EphemeralClient()\n",
    "# si se quiere hacer con persistencia en disco\n",
    "# hago la consulta de las n_cercanas\n",
    "n_cercanas = 2500\n",
    "#embedding_name = \"OpenAI\" # nombre del modelo de embeddings, por si se quieren probar varios\n",
    "#embedding_name = \"HF_MiniLM\" # nombre del modelo de embeddings MiniLM\n",
    "embedding_name = \"HF_mpnet\" # nombre del modelo de embeddings mpnet\n",
    "\n",
    "# para OpenAI no es necesario inicializar el modelo\n",
    "if embedding_name == \"HF_MiniLM\":\n",
    "    nombre_modelo = \"all-MiniLM-L6-v2\"  # mas ligero pero menos preciso\n",
    "    model = SentenceTransformer(nombre_modelo)\n",
    "\n",
    "if embedding_name == \"HF_mpnet\":\n",
    "    nombre_modelo = \"sentence-transformers/all-mpnet-base-v2\"  # mas pesado pero mas preciso\n",
    "    model = SentenceTransformer(nombre_modelo)\n",
    "\n",
    "\n",
    "path_DB = \"../kaggle_AILA/chroma_db_casedocs_\"+embedding_name\n",
    "chroma_client = chromadb.PersistentClient(path=path_DB)  \n",
    "\n",
    "collection = chroma_client.get_or_create_collection(name=embedding_name)\n",
    "\n",
    "fichero_resultados = f\"\"\"../resultados/embeddings_casedocs/resultados_embeddings_{embedding_name}_{n_cercanas}_cercanas.txt\"\"\"\n",
    "# abre el fichero de resultados de escritura y escribe la cabecera\n",
    "\n",
    "with open(fichero_resultados, \"w\") as file:\n",
    "    file.write(f\"\"\"Resultados de las consultas con embeddings casedocs de {embedding_name} de las {n_cercanas} mas cercanas:\"\"\")\n",
    "    file.write(\"\\n--------------------------------------------------------\\n\")\n",
    "    file.write(\"\\n\")\n",
    "\n",
    "# inicializa la precisión acumulada\n",
    "precision_acumulada = 0\n",
    "# recorre todas las querys y para cada query hace una consulta con el texto de la query\n",
    "# y obtiene las n_cercanas casedocs\n",
    "# en cada query se compara las casedocs obtenidas con las que se encuentran en s_relevance_dict\n",
    "# y se calcula el número de aciertos\n",
    "for query in query_dict:\n",
    "    # obtener el texto de la query del diccionario\n",
    "    query_text=query_dict[query]\n",
    "    # Calculo el embedding del texto de la query\n",
    "    if embedding_name == \"HF_MiniLM\" or embedding_name == \"HF_mpnet\":\n",
    "        query_embedding = model.encode([query_text]).tolist()[0]\n",
    "    elif embedding_name == \"OpenAI\":\n",
    "        query_embedding = get_embedding_OpenAI(query_text)\n",
    "\n",
    "    results = collection.query(query_embeddings=[query_embedding], n_results=n_cercanas)\n",
    "\n",
    "    aciertos = 0\n",
    "    for casedoc in c_relevance_dict[query]:\n",
    "        if casedoc in results['ids'][0]:\n",
    "            aciertos += 1\n",
    "    # calculo la precisión\n",
    "    precision = aciertos / len(c_relevance_dict[query]) * 100\n",
    "    precision_acumulada += precision\n",
    "\n",
    "    # añadimos a un fichero los resultados\n",
    "    resultado = f\"\"\"\n",
    "    Query: {query}\n",
    "    Casos relevantes: {c_relevance_dict[query]}\n",
    "    {n_cercanas} casos más cercanos: {results['ids'][0]}\n",
    "    Número de aciertos: {aciertos}\n",
    "    Precisión: {precision}\n",
    "    \"\"\"\n",
    "    with open(fichero_resultados, \"a\") as file:\n",
    "        file.write(resultado)\n",
    "\n",
    "\n",
    "    # si quisieramos imprimir los resultados de los statutes con sus textos debemos\n",
    "    # tener en cuenta que es un diccionario en el que los nombres de statues están en el 'ids'\n",
    "    # y los textos en 'documents' \n",
    "    # for i in range(len(results[\"documents\"][0])):\n",
    "    #     print(f\"{i+1}. Statute: {results['ids'][0][i]} - Texto: {results['documents'][0][i]}\")\n",
    "\n",
    "\n",
    "# calculo la precisión media\n",
    "precision_media = precision_acumulada / len(query_dict)\n",
    "# añado al fichero de resultados la precisión media\n",
    "with open(fichero_resultados, \"a\") as file:\n",
    "    file.write(f\"\\n*************************************\\n\")\n",
    "    file.write(f\"Precision media: {precision_media}\")\n",
    "# imprimo por consola el fichero\n",
    "with open(fichero_resultados, \"r\") as file:\n",
    "    print(file.read())\n",
    "#cierro el fichero de resultados\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# Inicializar ChromaDB en memoria (sin persistencia en disco)\n",
    "# chroma_client = chromadb.EphemeralClient()\n",
    "# si se quiere hacer con persistencia en disco\n",
    "# hago la consulta de las n_cercanas\n",
    "n_cercanas = 197\n",
    "#embedding_name = \"OpenAI\" # nombre del modelo de embeddings, por si se quieren probar varios\n",
    "#embedding_name = \"HF_MiniLM\" # nombre del modelo de embeddings MiniLM\n",
    "#embedding_name = \"HF_mpnet\" # nombre del modelo de embeddings mpnet\n",
    "embedding_name = \"HF_mpnet_finetunning\" # nombre del modelo de embeddings mpnet re-entrenado\n",
    "\n",
    "# para OpenAI no es necesario inicializar el modelo\n",
    "if embedding_name == \"HF_MiniLM\":\n",
    "    nombre_modelo = \"all-MiniLM-L6-v2\"  # mas ligero pero menos preciso\n",
    "    model = SentenceTransformer(nombre_modelo)\n",
    "\n",
    "if embedding_name == \"HF_mpnet\":\n",
    "    nombre_modelo = \"sentence-transformers/all-mpnet-base-v2\"  # mas pesado pero mas preciso\n",
    "    model = SentenceTransformer(nombre_modelo)\n",
    "\n",
    "if embedding_name == \"HF_mpnet_finetunning\":\n",
    "    nombre_modelo = \"../modelos_fine_tunning/sentence-transformers/all-mpnet-base-v2_finetunning\"  # mas pesado pero mas preciso\n",
    "    model = SentenceTransformer(nombre_modelo)\n",
    "\n",
    "\n",
    "path_DB = \"../chroma_db_\"+embedding_name\n",
    "chroma_client = chromadb.PersistentClient(path=path_DB)  \n",
    "\n",
    "collection = chroma_client.get_or_create_collection(name=embedding_name)\n",
    "\n",
    "fichero_resultados = f\"\"\"../resultados/embeddings/resultados_embeddings_{embedding_name}_{n_cercanas}_cercanas.txt\"\"\"\n",
    "# abre el fichero de resultados de escritura y escribe la cabecera\n",
    "\n",
    "with open(fichero_resultados, \"w\") as file:\n",
    "    file.write(f\"\"\"Resultados de las consultas con embeddings de {embedding_name} de las {n_cercanas} mas cercanas:\"\"\")\n",
    "    file.write(\"\\n--------------------------------------------------------\\n\")\n",
    "    file.write(\"\\n\")\n",
    "\n",
    "# inicializa la precisión acumulada\n",
    "precision_acumulada = 0\n",
    "# recorre todas las querys y para cada query hace una consulta con el texto de la query\n",
    "# y obtiene las n_cercanas statutes\n",
    "# en cada query se compara las statutes obtenidas con las que se encuentran en s_relevance_dict\n",
    "# y se calcula el número de aciertos\n",
    "for query in query_dict:\n",
    "    # obtener el texto de la query del diccionario\n",
    "    query_text=query_dict[query]\n",
    "    # Calculo el embedding del texto de la query\n",
    "    if embedding_name == \"HF_MiniLM\" or embedding_name == \"HF_mpnet\" or embedding_name == \"HF_mpnet_finetunning\":\n",
    "        query_embedding = model.encode([query_text]).tolist()[0]\n",
    "    elif embedding_name == \"OpenAI\":\n",
    "        query_embedding = get_embedding_OpenAI(query_text)\n",
    "\n",
    "    results = collection.query(query_embeddings=[query_embedding], n_results=n_cercanas)\n",
    "\n",
    "    aciertos = 0\n",
    "    for statute in s_relevance_dict[query]:\n",
    "        if statute in results['ids'][0]:\n",
    "            aciertos += 1\n",
    "    # calculo la precisión\n",
    "    precision = aciertos / len(s_relevance_dict[query]) * 100\n",
    "    precision_acumulada += precision\n",
    "\n",
    "    # añadimos a un fichero los resultados\n",
    "    resultado = f\"\"\"\n",
    "    Query: {query}\n",
    "    Statutes relevantes: {s_relevance_dict[query]}\n",
    "    {n_cercanas} Statutes más cercanos: {results['ids'][0]}\n",
    "    Número de aciertos: {aciertos}\n",
    "    Precisión: {precision}\n",
    "    \"\"\"\n",
    "    with open(fichero_resultados, \"a\") as file:\n",
    "        file.write(resultado)\n",
    "\n",
    "\n",
    "    # si quisieramos imprimir los resultados de los statutes con sus textos debemos\n",
    "    # tener en cuenta que es un diccionario en el que los nombres de statues están en el 'ids'\n",
    "    # y los textos en 'documents' \n",
    "    # for i in range(len(results[\"documents\"][0])):\n",
    "    #     print(f\"{i+1}. Statute: {results['ids'][0][i]} - Texto: {results['documents'][0][i]}\")\n",
    "\n",
    "\n",
    "# calculo la precisión media\n",
    "precision_media = precision_acumulada / len(query_dict)\n",
    "# añado al fichero de resultados la precisión media\n",
    "with open(fichero_resultados, \"a\") as file:\n",
    "    file.write(f\"\\n*************************************\\n\")\n",
    "    file.write(f\"Precision media: {precision_media}\")\n",
    "# imprimo por consola el fichero\n",
    "with open(fichero_resultados, \"r\") as file:\n",
    "    print(file.read())\n",
    "#cierro el fichero de resultados\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# recorre todos los casedocs, y para cada casedoc recorre todas las queries\n",
    "# si el casedoc es relevante para la query crea una terna con el texto del casedoc,\n",
    "# el texto de la query y un 1 si es relevante o un 0 si no es relevante. Añade la terna\n",
    "# a una lista de ternas. Al final crea un fichero con las ternas.\n",
    "file_path = \"../dataset_entrenamiento\"\n",
    "file_name = file_path + \"/tripletas_casedocs_queries.txt\"\n",
    "\n",
    "dataset_t = []\n",
    "for c in c_dict:\n",
    "    for q in query_dict:\n",
    "        relevante = 1 if c in c_relevance_dict[q] else 0\n",
    "        # crea una tupla de la forma (c_dict[c], query_dict[q], relevante)\n",
    "        terna = (c_dict[c], query_dict[q], relevante)\n",
    "        # añade la terna a la lista de ternas\n",
    "        dataset_t.append(terna)\n",
    "        \n",
    "# crea un dataset de entrenamiento equilibrado con el mismo número de ternas\n",
    "# relevantes que no relevantes a partir del diccionario c_relevance_dict\n",
    "# y el diccionario c_dict. Para cada casedoc relevante se selecciona un casedoc\n",
    "# no relevante al azar y crea una terna con el texto del casedoc, el texto de la query\n",
    "# y un 1 si es relevante o un 0 si no es relevante. Añade la terna a una lista de ternas.\n",
    "dataset_eq_t = []\n",
    "for q in query_dict:\n",
    "    for c in c_relevance_dict[q]:\n",
    "        relevante = 1\n",
    "        # selecciona un casedoc no relevante al azar\n",
    "        c_no_relevante = random.choice(list(set(c_dict.keys()) - set(c_relevance_dict[q])))\n",
    "        # crea una tupla de la forma (c_dict[c_no_relevante], query_dict[q], 0)\n",
    "        terna = (c_dict[c_no_relevante], query_dict[q], 0)\n",
    "        # añade la terna a la lista de ternas\n",
    "        dataset_eq_t.append(terna)\n",
    "        # crea una tupla de la forma (c_dict[c], query_dict[q], 1)\n",
    "        terna = (c_dict[c], query_dict[q], 1)\n",
    "        # añade la terna a la lista de ternas\n",
    "        dataset_eq_t.append(terna)\n",
    "\n",
    "# baraja el dataset de ternas\n",
    "random.shuffle(dataset_t)\n",
    "# baraja el dataset de ternas equilibrado\n",
    "#random.shuffle(dataset_eq_t)\n",
    "# imprime las dimensiones de los datasets y la forma que tienen\n",
    "print(f\"Dimensiones del dataset de ternas: {len(dataset_t)}\")\n",
    "print(f\"Dimensiones del dataset de ternas equilibrado: {len(dataset_eq_t)}\")\n",
    "# imprime las primeras ternas del data set equilibrado\n",
    "for i in range(5):\n",
    "    print(dataset_eq_t[i])\n",
    "# salva a disco cada dataset de ternas para que puedan ser recuperadas en el mismo formato\n",
    "# para futuros entrenamientos con pickle\n",
    "file_name = file_path + \"/tripletas_casedocs_queries.pkl\"\n",
    "with open(file_name, \"wb\") as file:\n",
    "    pickle.dump(dataset_t, file)\n",
    "file.close()\n",
    "file_name = file_path + \"/tripletas_casedocs_queries_equilibrado.pkl\"\n",
    "with open(file_name, \"wb\") as file:\n",
    "    pickle.dump(dataset_eq_t, file)\n",
    "file.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ULL-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
